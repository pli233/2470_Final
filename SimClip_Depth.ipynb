{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision.models as models\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "if device.type == \"cuda\":\n",
    "    print(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    print(\"Using CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using pretrained ResNet50\n",
    "class ResNetFeatureExtractor(nn.Module):\n",
    "    def __init__(self, input_channels):\n",
    "        super(ResNetFeatureExtractor, self).__init__()\n",
    "        if input_channels == 3:\n",
    "            self.resnet = models.resnet18(pretrained=True)\n",
    "        else:\n",
    "            self.resnet = models.resnet18(pretrained=True)\n",
    "            self.resnet.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.resnet = nn.Sequential(*list(self.resnet.children())[:-1])  # Remove the last fully connected layer\n",
    "        self.fc = nn.Linear(512, 256)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.resnet(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "# Define two networks for RGB and depth images\n",
    "rgb_net = ResNetFeatureExtractor(input_channels=3).to(device)\n",
    "depth_net = ResNetFeatureExtractor(input_channels=1).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define projection layers to project features into a common alignment space\n",
    "class ProjectionHead(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(ProjectionHead, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        return x\n",
    "\n",
    "projection_rgb = ProjectionHead(256, 128).to(device)\n",
    "projection_depth = ProjectionHead(256, 128).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 定义对比损失函数（如CLIP中常用的对比损失）\n",
    "def contrastive_loss(features1, features2):\n",
    "    # L2 正则化\n",
    "    features1 = F.normalize(features1, p=2, dim=1)\n",
    "    features2 = F.normalize(features2, p=2, dim=1)\n",
    "    # 计算相似度矩阵\n",
    "    logits = torch.matmul(features1, features2.T)\n",
    "    labels = torch.arange(features1.size(0)).to(features1.device)\n",
    "    loss = F.cross_entropy(logits, labels)\n",
    "    \n",
    "    return loss\n",
    "\n",
    "# Define the NT-Xent Loss (used in SimCLR's contrastive loss)\n",
    "def nt_xent_loss(features1, features2, temperature=0.5):\n",
    "    # L2 normalization\n",
    "    features1 = F.normalize(features1, p=2, dim=1)\n",
    "    features2 = F.normalize(features2, p=2, dim=1)\n",
    "    \n",
    "    # Concatenate features\n",
    "    features = torch.cat([features1, features2], dim=0)\n",
    "    \n",
    "    # Compute similarity matrix\n",
    "    similarity_matrix = torch.matmul(features, features.T) / temperature\n",
    "    \n",
    "    # Create labels\n",
    "    batch_size = features1.size(0)\n",
    "    labels = torch.arange(batch_size).to(device)\n",
    "    labels = torch.cat([labels, labels], dim=0)\n",
    "    \n",
    "    # Remove diagonal elements (self-comparison)\n",
    "    mask = torch.eye(2 * batch_size, dtype=bool).to(device)\n",
    "    similarity_matrix = similarity_matrix.masked_fill(mask, -float('inf'))\n",
    "    \n",
    "    # Compute loss\n",
    "    positives = torch.cat([torch.diag(similarity_matrix, batch_size), torch.diag(similarity_matrix, -batch_size)])\n",
    "    negatives = similarity_matrix[~mask].view(2 * batch_size, -1)\n",
    "    logits = torch.cat([positives.unsqueeze(1), negatives], dim=1)\n",
    "    \n",
    "    labels = torch.zeros(2 * batch_size, dtype=torch.long).to(device)\n",
    "    loss = F.cross_entropy(logits, labels)\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the multimodal dataset\n",
    "class MultiModalDataset(Dataset):\n",
    "    def __init__(self, rgb_root, depth_root, transform_rgb, transform_rgb_aug, transform_depth, transform_depth_aug):\n",
    "        # Load RGB and depth datasets\n",
    "        self.rgb_dataset = datasets.ImageFolder(root=rgb_root, transform=transform_rgb)\n",
    "        self.rgb_augmented_dataset = datasets.ImageFolder(root=rgb_root, transform=transform_rgb_aug)\n",
    "        self.depth_dataset = datasets.ImageFolder(root=depth_root, transform=transform_depth)\n",
    "        self.depth_augmented_dataset = datasets.ImageFolder(root=depth_root, transform=transform_depth_aug)\n",
    "        \n",
    "        # Ensure all datasets have the same length\n",
    "        assert len(self.rgb_dataset) == len(self.rgb_augmented_dataset) == len(self.depth_dataset) == len(self.depth_augmented_dataset), \"Datasets have different lengths\"\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.rgb_dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        rgb_image, label = self.rgb_dataset[idx]\n",
    "        rgb_aug_image, _ = self.rgb_augmented_dataset[idx]\n",
    "        depth_image, _ = self.depth_dataset[idx]\n",
    "        depth_aug_image, _ = self.depth_augmented_dataset[idx]\n",
    "        return rgb_image, rgb_aug_image, depth_image, depth_aug_image, label\n",
    "\n",
    "\n",
    "\n",
    "# Define data augmentation transformations (SimCLR style)\n",
    "transform_rgb_augment = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(224),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ColorJitter(0.4, 0.4, 0.4, 0.4),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "transform_depth_augment = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=1),\n",
    "    transforms.RandomResizedCrop(224),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Basic transformation\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Basic transformation\n",
    "transform_gray = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=1),\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Load the dataset\n",
    "multi_modal_dataset = MultiModalDataset(\n",
    "    rgb_root='./affectnet_3750subset/train',\n",
    "    depth_root='./affectnet_3750subset/train',\n",
    "    transform_rgb=transform,\n",
    "    transform_rgb_aug=transform_rgb_augment,\n",
    "    transform_depth=transform_gray,\n",
    "    transform_depth_aug=transform_depth_augment\n",
    ")\n",
    "\n",
    "multi_modal_loader = DataLoader(multi_modal_dataset, batch_size=64, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Start training\")\n",
    "# Optimizer\n",
    "optimizer = optim.Adam(list(rgb_net.parameters()) + list(depth_net.parameters()) +\n",
    "                       list(projection_rgb.parameters()) + list(projection_depth.parameters()), lr=1e-4)\n",
    "\n",
    "# Training\n",
    "rgb_net.train()\n",
    "depth_net.train()\n",
    "projection_rgb.train()\n",
    "projection_depth.train()\n",
    "\n",
    "epochs = 24\n",
    "for epoch in range(epochs):\n",
    "    epoch_loss = 0\n",
    "    for rgb_images, rgb_augmented_images, depth_images, depth_augmented_images, labelq in tqdm(multi_modal_loader, desc=f\"Epoch [{epoch+1}/{epochs}]\"):\n",
    "        # Move data to GPU\n",
    "        rgb_images = rgb_images.to(device)\n",
    "        rgb_augmented_images = rgb_augmented_images.to(device)\n",
    "        depth_images = depth_images.to(device)\n",
    "        depth_augmented_images = depth_augmented_images.to(device)\n",
    "\n",
    "        # Contrastive learning with augmented images (RGB)\n",
    "        rgb_features_1 = rgb_net(rgb_augmented_images)\n",
    "        rgb_features_2 = rgb_net(rgb_images)\n",
    "        rgb_projection_1 = projection_rgb(rgb_features_1)\n",
    "        rgb_projection_2 = projection_rgb(rgb_features_2)\n",
    "        loss_rgb = nt_xent_loss(rgb_projection_1, rgb_projection_2)\n",
    "\n",
    "        # Contrastive learning with augmented images (Depth)\n",
    "        depth_features_1 = depth_net(depth_augmented_images)\n",
    "        depth_features_2 = depth_net(depth_images)\n",
    "        depth_projection_1 = projection_depth(depth_features_1)\n",
    "        depth_projection_2 = projection_depth(depth_features_2)\n",
    "        loss_depth = nt_xent_loss(depth_projection_1, depth_projection_2)\n",
    "\n",
    "        # Aligning RGB and depth images through contrastive learning\n",
    "        rgb_features = rgb_net(rgb_images)\n",
    "        depth_features = depth_net(depth_images)\n",
    "        rgb_projection = projection_rgb(rgb_features)\n",
    "        depth_projection = projection_depth(depth_features)\n",
    "        loss_rgb_depth = contrastive_loss(rgb_projection, depth_projection)\n",
    "\n",
    "        # Total loss\n",
    "        loss = loss_rgb + loss_depth + loss_rgb_depth\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "        # Backpropagation and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f'Epoch [{epoch+1}/{epochs}], Average Loss: {epoch_loss / len(multi_modal_loader)}')\n",
    "    \n",
    "    if epoch % 4 == 0:\n",
    "        # Save the model\n",
    "        torch.save({\n",
    "            'rgb_net_state_dict': rgb_net.state_dict(),\n",
    "            'depth_net_state_dict': depth_net.state_dict(),\n",
    "            'projection_rgb_state_dict': projection_rgb.state_dict(),\n",
    "            'projection_depth_state_dict': projection_depth.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "        }, f'contrastive_learning_model_epoch_{epoch+1}.pth')\n",
    "\n",
    "    epoch += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load checkpoint from training phase\n",
    "load_epoch = 21\n",
    "checkpoint = torch.load(f'contrastive_learning_model_epoch_{load_epoch}.pth')\n",
    "rgb_net.load_state_dict(checkpoint['rgb_net_state_dict'])\n",
    "depth_net.load_state_dict(checkpoint['depth_net_state_dict'])\n",
    "projection_rgb.load_state_dict(checkpoint['projection_rgb_state_dict'])\n",
    "projection_depth.load_state_dict(checkpoint['projection_depth_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "print(f'Model from epoch {load_epoch} loaded successfully.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiModalDataset_Test(Dataset):\n",
    "    def __init__(self, rgb_root, depth_root, transform_rgb, transform_depth):\n",
    "        # 加载 RGB 和深度数据集\n",
    "        self.rgb_dataset = datasets.ImageFolder(root=rgb_root, transform=transform_rgb)\n",
    "        self.depth_dataset = datasets.ImageFolder(root=depth_root, transform=transform_depth)\n",
    "        \n",
    "        # 确保所有数据集长度相同\n",
    "        print(len(self.rgb_dataset))\n",
    "        print(len(self.depth_dataset))\n",
    "        assert len(self.rgb_dataset) == len(self.depth_dataset), \"Datasets have different lengths\"\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.rgb_dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        rgb_image, label = self.rgb_dataset[idx]\n",
    "        depth_image, _ = self.depth_dataset[idx]\n",
    "        return rgb_image, depth_image, label\n",
    "    \n",
    "multi_modal_dataset_test = MultiModalDataset_Test(\n",
    "    rgb_root='./affectnet_3750subset/test',\n",
    "    depth_root='./affectnet_3750subset/test',\n",
    "    transform_rgb=transform,\n",
    "    transform_depth=transform_gray,\n",
    ")\n",
    "\n",
    "multi_modal_loader_test = DataLoader(multi_modal_dataset_test, batch_size=64, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finetune 阶段\n",
    "print(\"Starting Finetune Phase...\")\n",
    "finetune_epochs = 5\n",
    "classifier = nn.Sequential(\n",
    "    nn.Linear(512, 128),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(128, 8)  # 假设有8个类别\n",
    ").cuda()\n",
    "\n",
    "finetune_optimizer = optim.Adam(list(rgb_net.parameters()) + list(depth_net.parameters()) + list(classifier.parameters()), lr=1e-4)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch in range(finetune_epochs):\n",
    "    epoch_loss = 0\n",
    "    for rgb_images, rgb_augmented_images, depth_images, depth_augmented_images, label_y in tqdm(multi_modal_loader, desc=f\"Epoch [{epoch+1}/{finetune_epochs}]\"):\n",
    "        # 将数据移动到GPU\n",
    "        rgb_images = rgb_images.cuda()\n",
    "        depth_images = depth_images.cuda()\n",
    "\n",
    "        label_y = label_y.cuda()\n",
    "\n",
    "        # 提取投影特征并 concatenate\n",
    "        rgb_features_2 = rgb_net(rgb_images)\n",
    "        depth_features_2 = depth_net(depth_images)\n",
    "\n",
    "        combined_features = torch.cat((rgb_features_2, depth_features_2), dim=1)\n",
    "\n",
    "        # 通过分类器进行分类\n",
    "        outputs = classifier(combined_features)\n",
    "        loss = criterion(outputs, label_y)\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "        # 反向传播和优化\n",
    "        finetune_optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        finetune_optimizer.step()\n",
    "    print(f'Finetune Epoch [{epoch+1}/{finetune_epochs}], Average Loss: {epoch_loss / len(multi_modal_loader)}')\n",
    "\n",
    "    # 测试集上的评估\n",
    "    rgb_net.eval()\n",
    "    depth_net.eval()\n",
    "    classifier.eval()\n",
    "    projection_rgb.eval()\n",
    "    projection_depth.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for rgb_images_test, depth_images_test, label_y_test in tqdm(multi_modal_loader_test, desc=f\"Epoch [{epoch+1}/{finetune_epochs}]\"):\n",
    "            # 将数据移动到GPU\n",
    "            rgb_images_test = rgb_images_test.cuda()\n",
    "            depth_images_test = depth_images_test.cuda()\n",
    "            label_y_test = label_y_test.cuda()\n",
    "            # print(labels)\n",
    "            # print(\"********************\")\n",
    "            # print(labels_depth)\n",
    "\n",
    "            # 提取投影特征并 concatenate\n",
    "            rgb_features_2 = rgb_net(rgb_images_test)\n",
    "            depth_features_2 = depth_net(depth_images_test)\n",
    "\n",
    "            # print(rgb_features_2.shape)\n",
    "            # print(depth_features_2.shape)\n",
    "            combined_features = torch.cat((rgb_features_2, depth_features_2), dim=1)\n",
    "\n",
    "            # 通过分类器进行分类\n",
    "            outputs = classifier(combined_features)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            # print(predicted)\n",
    "            total += label_y_test.size(0)\n",
    "            correct += (predicted == label_y_test).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f'Test Accuracy after Finetune Epoch [{epoch+1}/{finetune_epochs}]: {accuracy:.2f}%')\n",
    "\n",
    "    rgb_net.train()\n",
    "    depth_net.train()\n",
    "    classifier.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
