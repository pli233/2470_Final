{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision.models as models\n",
    "from tqdm import tqdm\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU: NVIDIA GeForce RTX 3090\n"
     ]
    }
   ],
   "source": [
    "# Device configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "if device.type == \"cuda\":\n",
    "    print(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    print(\"Using CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/peiyuan/miniconda3/envs/pytorch-gpu/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/peiyuan/miniconda3/envs/pytorch-gpu/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# Using pretrained ResNet50\n",
    "class ResNetFeatureExtractor(nn.Module):\n",
    "    def __init__(self, input_channels):\n",
    "        super(ResNetFeatureExtractor, self).__init__()\n",
    "        if input_channels == 3:\n",
    "            self.resnet = models.resnet18(pretrained=True)\n",
    "        else:\n",
    "            self.resnet = models.resnet18(pretrained=True)\n",
    "            self.resnet.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.resnet = nn.Sequential(*list(self.resnet.children())[:-1])  # Remove the last fully connected layer\n",
    "        self.fc = nn.Linear(512, 256)\n",
    "        # self.fc = nn.Linear(2048, 256)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.resnet(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "# Define two networks for RGB and depth images\n",
    "rgb_net = ResNetFeatureExtractor(input_channels=3).to(device)\n",
    "depth_net = ResNetFeatureExtractor(input_channels=1).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define projection layers to project features into a common alignment space\n",
    "class ProjectionHead(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(ProjectionHead, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        return x\n",
    "\n",
    "projection_rgb = ProjectionHead(256, 128).to(device)\n",
    "projection_depth = ProjectionHead(256, 128).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 定义对比损失函数（如CLIP中常用的对比损失）\n",
    "def contrastive_loss(features1, features2):\n",
    "    # L2 正则化\n",
    "    features1 = F.normalize(features1, p=2, dim=1)\n",
    "    features2 = F.normalize(features2, p=2, dim=1)\n",
    "    # 计算相似度矩阵\n",
    "    logits = torch.matmul(features1, features2.T)\n",
    "    labels = torch.arange(features1.size(0)).to(features1.device)\n",
    "    loss = F.cross_entropy(logits, labels)\n",
    "    \n",
    "    return loss\n",
    "\n",
    "# Define the NT-Xent Loss (used in SimCLR's contrastive loss)\n",
    "def nt_xent_loss(features1, features2, temperature=0.5):\n",
    "    # L2 normalization\n",
    "    features1 = F.normalize(features1, p=2, dim=1)\n",
    "    features2 = F.normalize(features2, p=2, dim=1)\n",
    "    \n",
    "    # Concatenate features\n",
    "    features = torch.cat([features1, features2], dim=0)\n",
    "    \n",
    "    # Compute similarity matrix\n",
    "    similarity_matrix = torch.matmul(features, features.T) / temperature\n",
    "    \n",
    "    # Create labels\n",
    "    batch_size = features1.size(0)\n",
    "    labels = torch.arange(batch_size).to(device)\n",
    "    labels = torch.cat([labels, labels], dim=0)\n",
    "    \n",
    "    # Remove diagonal elements (self-comparison)\n",
    "    mask = torch.eye(2 * batch_size, dtype=bool).to(device)\n",
    "    similarity_matrix = similarity_matrix.masked_fill(mask, -float('inf'))\n",
    "    \n",
    "    # Compute loss\n",
    "    positives = torch.cat([torch.diag(similarity_matrix, batch_size), torch.diag(similarity_matrix, -batch_size)])\n",
    "    negatives = similarity_matrix[~mask].view(2 * batch_size, -1)\n",
    "    logits = torch.cat([positives.unsqueeze(1), negatives], dim=1)\n",
    "    \n",
    "    labels = torch.zeros(2 * batch_size, dtype=torch.long).to(device)\n",
    "    loss = F.cross_entropy(logits, labels)\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from affectnet_util import AffectDataSet\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class MultiModalDataset(Dataset):\n",
    "    def __init__(self, data_path, affcls, train, exclude_classes, \n",
    "                 transform_rgb, transform_rgb_aug, transform_depth, transform_depth_aug):\n",
    "        self.train=train\n",
    "        self.transform_rgb = transform_rgb\n",
    "        self.transform_rgb_aug = transform_rgb_aug\n",
    "        self.transform_depth = transform_depth\n",
    "        self.transform_depth_aug = transform_depth_aug\n",
    "\n",
    "        # 使用 AffectDataSet 加载 RGB 和深度数据\n",
    "        self.rgb_dataset = AffectDataSet(data_path=data_path, train= train, affcls=affcls, \n",
    "                                         transform=self.transform_rgb, exclude_classes=exclude_classes)\n",
    "        self.rgb_augmented_dataset = AffectDataSet(data_path=data_path, train= train, affcls=affcls, \n",
    "                                                   transform=self.transform_rgb_aug, exclude_classes=exclude_classes)\n",
    "        self.depth_dataset = AffectDataSet(data_path=data_path, train= train, affcls=affcls, \n",
    "                                           transform=self.transform_depth, exclude_classes=exclude_classes)\n",
    "        self.depth_augmented_dataset = AffectDataSet(data_path=data_path, train= train, affcls=affcls, \n",
    "                                                     transform=self.transform_depth_aug, exclude_classes=exclude_classes)\n",
    "\n",
    "        # 确保数据长度一致\n",
    "        assert len(self.rgb_dataset) == len(self.rgb_augmented_dataset) == len(self.depth_dataset) == len(self.depth_augmented_dataset), \\\n",
    "            \"Datasets have different lengths\"\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.rgb_dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # 获取 RGB 图像及其增强版本\n",
    "        rgb_image, label = self.rgb_dataset[idx]\n",
    "        rgb_aug_image, _ = self.rgb_augmented_dataset[idx]\n",
    "\n",
    "        # 获取深度图像及其增强版本\n",
    "        depth_image, _ = self.depth_dataset[idx]\n",
    "        depth_aug_image, _ = self.depth_augmented_dataset[idx]\n",
    "\n",
    "        return rgb_image, rgb_aug_image, depth_image, depth_aug_image, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribution of train samples: [ 74874 134415  25459  14090  24882]\n",
      "Distribution of train samples: [ 74874 134415  25459  14090  24882]\n",
      "Distribution of train samples: [ 74874 134415  25459  14090  24882]\n",
      "Distribution of train samples: [ 74874 134415  25459  14090  24882]\n",
      "RGB image shape: torch.Size([32, 3, 224, 224])\n",
      "Depth image shape: torch.Size([32, 1, 224, 224])\n",
      "Labels: tensor([1, 0, 2, 1, 1, 6, 0, 1, 6, 3, 1, 3, 3, 3, 1, 1, 1, 1, 2, 1, 1, 0, 1, 0,\n",
      "        1, 0, 1, 1, 1, 0, 6, 2])\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "# 定义 RGB 和深度图像的增强及基本转换\n",
    "transform_rgb = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "transform_rgb_augment = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(224),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ColorJitter(0.4, 0.4, 0.4, 0.4),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "transform_depth = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=1),\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "transform_depth_augment = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=1),\n",
    "    transforms.RandomResizedCrop(224),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# 排除的类别\n",
    "exclude_classes = [4, 5]\n",
    "\n",
    "# 加载多模态训练集\n",
    "multi_modal_train_dataset = MultiModalDataset(\n",
    "    data_path=\"/CSCI2952X/datasets/affectnet\",\n",
    "    affcls=7,\n",
    "    train= True,\n",
    "    exclude_classes=exclude_classes,\n",
    "    transform_rgb=transform_rgb,\n",
    "    transform_rgb_aug=transform_rgb_augment,\n",
    "    transform_depth=transform_depth,\n",
    "    transform_depth_aug=transform_depth_augment\n",
    ")\n",
    "\n",
    "# 创建 DataLoader\n",
    "multi_modal_train_loader = DataLoader(multi_modal_train_dataset, batch_size=32, shuffle=True, num_workers=4)\n",
    "\n",
    "# 测试数据加载\n",
    "for rgb_image, rgb_aug_image, depth_image, depth_aug_image, label in multi_modal_train_loader:\n",
    "    print(f\"RGB image shape: {rgb_image.shape}\")  # torch.Size([32, 3, 224, 224])\n",
    "    print(f\"Depth image shape: {depth_image.shape}\")  # torch.Size([32, 1, 224, 224])\n",
    "    print(f\"Labels: {label}\")  # 检查标签是否正确\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [1/30]:   1%|          | 98/8554 [00:16<23:24,  6.02it/s] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 36\u001b[0m\n\u001b[1;32m     34\u001b[0m rgb_projection_1 \u001b[38;5;241m=\u001b[39m projection_rgb(rgb_features_1)\n\u001b[1;32m     35\u001b[0m rgb_projection_2 \u001b[38;5;241m=\u001b[39m projection_rgb(rgb_features_2)\n\u001b[0;32m---> 36\u001b[0m loss_rgb \u001b[38;5;241m=\u001b[39m \u001b[43mnt_xent_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrgb_projection_1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrgb_projection_2\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# Contrastive learning with augmented images (Depth)\u001b[39;00m\n\u001b[1;32m     39\u001b[0m depth_features_1 \u001b[38;5;241m=\u001b[39m depth_net(depth_augmented_images)\n",
      "Cell \u001b[0;32mIn[5], line 27\u001b[0m, in \u001b[0;36mnt_xent_loss\u001b[0;34m(features1, features2, temperature)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# Create labels\u001b[39;00m\n\u001b[1;32m     26\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m features1\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m---> 27\u001b[0m labels \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marange\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m labels \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([labels, labels], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# Remove diagonal elements (self-comparison)\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(\"Start training\")\n",
    "# Optimizer\n",
    "optimizer = optim.Adam(list(rgb_net.parameters()) + list(depth_net.parameters()) +\n",
    "                       list(projection_rgb.parameters()) + list(projection_depth.parameters()), lr=1e-3)\n",
    "\n",
    "# Define your experiment name and create save directory\n",
    "experiment_name = 'new_rn18_depth_finetune_50ep'\n",
    "save_dir = os.path.join('saved_models', experiment_name)\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "best_loss = float('inf')  # Initialize best loss to a large value\n",
    "best_model_path = os.path.join(save_dir, 'best_model.pth')\n",
    "latest_model_path = os.path.join(save_dir, 'latest_model.pth')\n",
    "\n",
    "# Training\n",
    "rgb_net.train()\n",
    "depth_net.train()\n",
    "projection_rgb.train()\n",
    "projection_depth.train()\n",
    "\n",
    "epochs = 30\n",
    "for epoch in range(epochs):\n",
    "    epoch_loss = 0\n",
    "    for rgb_images, rgb_augmented_images, depth_images, depth_augmented_images, labelq in tqdm(multi_modal_train_loader, desc=f\"Epoch [{epoch+1}/{epochs}]\"):\n",
    "        # Move data to GPU\n",
    "        rgb_images = rgb_images.to(device)\n",
    "        rgb_augmented_images = rgb_augmented_images.to(device)\n",
    "        depth_images = depth_images.to(device)\n",
    "        depth_augmented_images = depth_augmented_images.to(device)\n",
    "\n",
    "        # Contrastive learning with augmented images (RGB)\n",
    "        rgb_features_1 = rgb_net(rgb_augmented_images)\n",
    "        rgb_features_2 = rgb_net(rgb_images)\n",
    "        rgb_projection_1 = projection_rgb(rgb_features_1)\n",
    "        rgb_projection_2 = projection_rgb(rgb_features_2)\n",
    "        loss_rgb = nt_xent_loss(rgb_projection_1, rgb_projection_2)\n",
    "\n",
    "        # Contrastive learning with augmented images (Depth)\n",
    "        depth_features_1 = depth_net(depth_augmented_images)\n",
    "        depth_features_2 = depth_net(depth_images)\n",
    "        depth_projection_1 = projection_depth(depth_features_1)\n",
    "        depth_projection_2 = projection_depth(depth_features_2)\n",
    "        loss_depth = nt_xent_loss(depth_projection_1, depth_projection_2)\n",
    "\n",
    "        # Aligning RGB and depth images through contrastive learning\n",
    "        rgb_features = rgb_net(rgb_images)\n",
    "        depth_features = depth_net(depth_images)\n",
    "        rgb_projection = projection_rgb(rgb_features)\n",
    "        depth_projection = projection_depth(depth_features)\n",
    "        loss_rgb_depth = contrastive_loss(rgb_projection, depth_projection)\n",
    "\n",
    "        # Total loss\n",
    "        loss = loss_rgb + loss_depth + loss_rgb_depth\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "        # Backpropagation and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    average_loss = epoch_loss / len(multi_modal_train_loader)\n",
    "    print(f'Epoch [{epoch+1}/{epochs}], Average Loss: {average_loss}')\n",
    "    \n",
    "    # Save the latest model\n",
    "    torch.save({\n",
    "        'epoch': epoch + 1,\n",
    "        'rgb_net_state_dict': rgb_net.state_dict(),\n",
    "        'depth_net_state_dict': depth_net.state_dict(),\n",
    "        'projection_rgb_state_dict': projection_rgb.state_dict(),\n",
    "        'projection_depth_state_dict': projection_depth.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'avg_loss': average_loss,\n",
    "    }, latest_model_path)\n",
    "    print(f'Latest model saved to {latest_model_path}')\n",
    "    \n",
    "    # Save the best model\n",
    "    if average_loss < best_loss:\n",
    "        best_loss = average_loss\n",
    "        torch.save({\n",
    "            'epoch': epoch + 1,\n",
    "            'rgb_net_state_dict': rgb_net.state_dict(),\n",
    "            'depth_net_state_dict': depth_net.state_dict(),\n",
    "            'projection_rgb_state_dict': projection_rgb.state_dict(),\n",
    "            'projection_depth_state_dict': projection_depth.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'avg_loss': best_loss,\n",
    "        }, best_model_path)\n",
    "        print(f'New best model saved with loss {best_loss:.4f} to {best_model_path}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_52779/2791274589.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(best_model_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model from saved_models/new_rn18_depth_finetune_50ep/best_model.pth loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "#Load checkpoint from training phase\n",
    "checkpoint = torch.load(best_model_path)\n",
    "rgb_net.load_state_dict(checkpoint['rgb_net_state_dict'])\n",
    "depth_net.load_state_dict(checkpoint['depth_net_state_dict'])\n",
    "projection_rgb.load_state_dict(checkpoint['projection_rgb_state_dict'])\n",
    "projection_depth.load_state_dict(checkpoint['projection_depth_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "print(f'Model from {best_model_path} loaded successfully.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiModalDataset_Test(Dataset):\n",
    "    def __init__(self, data_path, affcls, train, transform_rgb, transform_depth, exclude_classes=None):\n",
    "        self.transform_rgb = transform_rgb\n",
    "        self.transform_depth = transform_depth\n",
    "\n",
    "        # 使用 AffectDataSet 加载 RGB 和深度数据\n",
    "        self.rgb_dataset = AffectDataSet(data_path=data_path, train = train, affcls=affcls, \n",
    "                                         transform=self.transform_rgb, exclude_classes=exclude_classes)\n",
    "        self.depth_dataset = AffectDataSet(data_path=data_path,  train = train, affcls=affcls, \n",
    "                                           transform=self.transform_depth, exclude_classes=exclude_classes)\n",
    "\n",
    "        # 确保数据长度一致\n",
    "        print(f\"RGB dataset size: {len(self.rgb_dataset)}\")\n",
    "        print(f\"Depth dataset size: {len(self.depth_dataset)}\")\n",
    "        assert len(self.rgb_dataset) == len(self.depth_dataset), \"Datasets have different lengths\"\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.rgb_dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # 获取 RGB 和深度图像\n",
    "        rgb_image, label = self.rgb_dataset[idx]\n",
    "        depth_image, _ = self.depth_dataset[idx]\n",
    "        return rgb_image, depth_image, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建测试集转换\n",
    "transform_rgb = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "transform_depth = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=1),\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# 要排除的类别\n",
    "exclude_classes = [4, 5]  # 假设我们需要过滤这些类别\n",
    "\n",
    "# 加载测试集\n",
    "multi_modal_dataset_test = MultiModalDataset_Test(\n",
    "    data_path=\"/CSCI2952X/datasets/affectnet\",\n",
    "    affcls=7,  # 假设是7类任务\n",
    "    train = False,\n",
    "    transform_rgb=transform_rgb,\n",
    "    transform_depth=transform_depth,\n",
    "    exclude_classes=exclude_classes\n",
    ")\n",
    "\n",
    "# 创建测试 DataLoader\n",
    "multi_modal_loader_test = DataLoader(multi_modal_dataset_test, batch_size=32, shuffle=False, num_workers=8)\n",
    "\n",
    "# 验证加载效果\n",
    "for rgb_image, depth_image, label in multi_modal_loader_test:\n",
    "    print(f\"RGB image shape: {rgb_image.shape}\")  # torch.Size([32, 3, 224, 224])\n",
    "    print(f\"Depth image shape: {depth_image.shape}\")  # torch.Size([32, 1, 224, 224])\n",
    "    print(f\"Labels: {label}\")  # 检查标签是否正确\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "print(\"Starting Finetune Phase...\")\n",
    "\n",
    "# Directory setup for saving fine-tuned models\n",
    "finetune_experiment_name = 'new_rn18_depth_finetune_50ep'\n",
    "finetune_save_dir = os.path.join('saved_models', finetune_experiment_name)\n",
    "os.makedirs(finetune_save_dir, exist_ok=True)\n",
    "\n",
    "best_finetune_loss = float('inf')  # Initialize best loss\n",
    "best_finetune_model_path = os.path.join(finetune_save_dir, 'best_finetune_model.pth')\n",
    "latest_finetune_model_path = os.path.join(finetune_save_dir, 'latest_finetune_model.pth')\n",
    "\n",
    "finetune_epochs = 30\n",
    "classifier = nn.Sequential(\n",
    "    nn.Linear(512, 128),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(128, 5)  # 128, x   x is number of class\n",
    ").cuda()\n",
    "\n",
    "finetune_optimizer = optim.Adam(list(rgb_net.parameters()) + list(depth_net.parameters()) + list(classifier.parameters()), lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch in range(finetune_epochs):\n",
    "    epoch_loss = 0\n",
    "    for rgb_images, rgb_augmented_images, depth_images, depth_augmented_images, label_y in tqdm(multi_modal_train_loader, desc=f\"Epoch [{epoch+1}/{finetune_epochs}]\"):\n",
    "        # 将数据移动到GPU\n",
    "        rgb_images = rgb_images.cuda()\n",
    "        depth_images = depth_images.cuda()\n",
    "        label_y = label_y.cuda()\n",
    "\n",
    "        # 提取投影特征并 concatenate\n",
    "        rgb_features_2 = rgb_net(rgb_images)\n",
    "        depth_features_2 = depth_net(depth_images)\n",
    "        combined_features = torch.cat((rgb_features_2, depth_features_2), dim=1)\n",
    "\n",
    "        # 通过分类器进行分类\n",
    "        outputs = classifier(combined_features)\n",
    "        loss = criterion(outputs, label_y)\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "        # 反向传播和优化\n",
    "        finetune_optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        finetune_optimizer.step()\n",
    "\n",
    "    average_loss = epoch_loss / len(multi_modal_train_loader)\n",
    "    print(f'Finetune Epoch [{epoch+1}/{finetune_epochs}], Average Loss: {average_loss}')\n",
    "\n",
    "    # 保存最新模型\n",
    "    torch.save({\n",
    "        'epoch': epoch + 1,\n",
    "        'rgb_net_state_dict': rgb_net.state_dict(),\n",
    "        'depth_net_state_dict': depth_net.state_dict(),\n",
    "        'classifier_state_dict': classifier.state_dict(),\n",
    "        'optimizer_state_dict': finetune_optimizer.state_dict(),\n",
    "        'avg_loss': average_loss,\n",
    "    }, latest_finetune_model_path)\n",
    "    print(f'Latest fine-tuned model saved to {latest_finetune_model_path}')\n",
    "\n",
    "    # 保存最佳模型\n",
    "    if average_loss < best_finetune_loss:\n",
    "        best_finetune_loss = average_loss\n",
    "        torch.save({\n",
    "            'epoch': epoch + 1,\n",
    "            'rgb_net_state_dict': rgb_net.state_dict(),\n",
    "            'depth_net_state_dict': depth_net.state_dict(),\n",
    "            'classifier_state_dict': classifier.state_dict(),\n",
    "            'optimizer_state_dict': finetune_optimizer.state_dict(),\n",
    "            'avg_loss': best_finetune_loss,\n",
    "        }, best_finetune_model_path)\n",
    "        print(f'New best fine-tuned model saved with loss {best_finetune_loss:.4f} to {best_finetune_model_path}')\n",
    "\n",
    "    # 测试集上的评估\n",
    "    rgb_net.eval()\n",
    "    depth_net.eval()\n",
    "    classifier.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for rgb_images_test, depth_images_test, label_y_test in tqdm(multi_modal_loader_test, desc=f\"Epoch [{epoch+1}/{finetune_epochs}]\"):\n",
    "            # 将数据移动到GPU\n",
    "            rgb_images_test = rgb_images_test.cuda()\n",
    "            depth_images_test = depth_images_test.cuda()\n",
    "            label_y_test = label_y_test.cuda()\n",
    "\n",
    "            # 提取投影特征并 concatenate\n",
    "            rgb_features_2 = rgb_net(rgb_images_test)\n",
    "            depth_features_2 = depth_net(depth_images_test)\n",
    "            combined_features = torch.cat((rgb_features_2, depth_features_2), dim=1)\n",
    "\n",
    "            # 通过分类器进行分类\n",
    "            outputs = classifier(combined_features)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += label_y_test.size(0)\n",
    "            correct += (predicted == label_y_test).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f'Test Accuracy after Finetune Epoch [{epoch+1}/{finetune_epochs}]: {accuracy:.2f}%')\n",
    "\n",
    "    rgb_net.train()\n",
    "    depth_net.train()\n",
    "    classifier.train()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
