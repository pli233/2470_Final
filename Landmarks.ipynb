{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f7c0c35-ee9b-47f2-9ca8-fca1e9b57971",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tqdm\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.7/57.7 kB\u001b[0m \u001b[31m683.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.5/78.5 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: tqdm\n",
      "Successfully installed tqdm-4.67.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Collecting albumentations==1.3.0\n",
      "  Downloading albumentations-1.3.0-py3-none-any.whl.metadata (34 kB)\n",
      "Collecting opencv-python\n",
      "  Downloading opencv_python-4.10.0.84-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
      "Requirement already satisfied: numpy>=1.11.1 in /usr/local/lib/python3.8/dist-packages (from albumentations==1.3.0) (1.24.4)\n",
      "Collecting scipy (from albumentations==1.3.0)\n",
      "  Downloading scipy-1.10.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (58 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.9/58.9 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting scikit-image>=0.16.1 (from albumentations==1.3.0)\n",
      "  Downloading scikit_image-0.21.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (14 kB)\n",
      "Requirement already satisfied: PyYAML in /usr/local/lib/python3.8/dist-packages (from albumentations==1.3.0) (6.0.1)\n",
      "Collecting qudida>=0.0.4 (from albumentations==1.3.0)\n",
      "  Downloading qudida-0.0.4-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting opencv-python-headless>=4.1.1 (from albumentations==1.3.0)\n",
      "  Downloading opencv_python_headless-4.10.0.84-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
      "Collecting scikit-learn>=0.19.1 (from qudida>=0.0.4->albumentations==1.3.0)\n",
      "  Downloading scikit_learn-1.3.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from qudida>=0.0.4->albumentations==1.3.0) (4.8.0)\n",
      "Collecting networkx>=2.8 (from scikit-image>=0.16.1->albumentations==1.3.0)\n",
      "  Downloading networkx-3.1-py3-none-any.whl.metadata (5.3 kB)\n",
      "Requirement already satisfied: pillow>=9.0.1 in /usr/local/lib/python3.8/dist-packages (from scikit-image>=0.16.1->albumentations==1.3.0) (10.1.0)\n",
      "Collecting imageio>=2.27 (from scikit-image>=0.16.1->albumentations==1.3.0)\n",
      "  Downloading imageio-2.35.1-py3-none-any.whl.metadata (4.9 kB)\n",
      "Collecting tifffile>=2022.8.12 (from scikit-image>=0.16.1->albumentations==1.3.0)\n",
      "  Downloading tifffile-2023.7.10-py3-none-any.whl.metadata (31 kB)\n",
      "Collecting PyWavelets>=1.1.1 (from scikit-image>=0.16.1->albumentations==1.3.0)\n",
      "  Downloading PyWavelets-1.4.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.9 kB)\n",
      "Requirement already satisfied: packaging>=21 in /usr/local/lib/python3.8/dist-packages (from scikit-image>=0.16.1->albumentations==1.3.0) (23.2)\n",
      "Collecting lazy_loader>=0.2 (from scikit-image>=0.16.1->albumentations==1.3.0)\n",
      "  Downloading lazy_loader-0.4-py3-none-any.whl.metadata (7.6 kB)\n",
      "Collecting joblib>=1.1.1 (from scikit-learn>=0.19.1->qudida>=0.0.4->albumentations==1.3.0)\n",
      "  Downloading joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting threadpoolctl>=2.0.0 (from scikit-learn>=0.19.1->qudida>=0.0.4->albumentations==1.3.0)\n",
      "  Downloading threadpoolctl-3.5.0-py3-none-any.whl.metadata (13 kB)\n",
      "Downloading albumentations-1.3.0-py3-none-any.whl (123 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m123.5/123.5 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading opencv_python-4.10.0.84-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (62.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hDownloading opencv_python_headless-4.10.0.84-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (49.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.9/49.9 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading qudida-0.0.4-py3-none-any.whl (3.5 kB)\n",
      "Downloading scikit_image-0.21.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.9/13.9 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading scipy-1.10.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34.5/34.5 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading imageio-2.35.1-py3-none-any.whl (315 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m315.4/315.4 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading lazy_loader-0.4-py3-none-any.whl (12 kB)\n",
      "Downloading networkx-3.1-py3-none-any.whl (2.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading PyWavelets-1.4.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading scikit_learn-1.3.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.1/11.1 MB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading tifffile-2023.7.10-py3-none-any.whl (220 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m220.9/220.9 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m301.8/301.8 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: tifffile, threadpoolctl, scipy, PyWavelets, opencv-python-headless, opencv-python, networkx, lazy_loader, joblib, imageio, scikit-learn, scikit-image, qudida, albumentations\n",
      "Successfully installed PyWavelets-1.4.1 albumentations-1.3.0 imageio-2.35.1 joblib-1.4.2 lazy_loader-0.4 networkx-3.1 opencv-python-4.10.0.84 opencv-python-headless-4.10.0.84 qudida-0.0.4 scikit-image-0.21.0 scikit-learn-1.3.2 scipy-1.10.1 threadpoolctl-3.5.0 tifffile-2023.7.10\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install tqdm\n",
    "!pip install albumentations==1.3.0 opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a8507f9-f8f4-46e6-8c9e-3ab69dfb2c4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RGB 数据集长度: 30000\n",
      "Landmarks 数据集长度: 29616\n",
      "RGB 路径样例: ['class_0/0', 'class_0/1', 'class_0/10', 'class_0/100', 'class_0/1000']\n",
      "Landmarks 路径样例: ['class_0/0', 'class_0/1000', 'class_0/1001', 'class_0/1002', 'class_0/1003']\n",
      "公共路径数量: 29616\n",
      "公共路径样例: ['class_5/3706', 'class_1/425', 'class_6/3362', 'class_5/1411', 'class_3/2641']\n",
      "同步后 RGB 数据集长度: 29616\n",
      "同步后 Landmarks 数据集长度: 29616\n",
      "RGB基本版: torch.Size([4, 3, 224, 224])\n",
      "RGB 4x增强马赛克: torch.Size([4, 3, 224, 224])\n",
      "Landmarks基本版: torch.Size([4, 3, 224, 224])\n",
      "Landmarks 4x增强马赛克: torch.Size([4, 3, 224, 224])\n",
      "标签: tensor([0, 7, 2, 4])\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import albumentations as A\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from simclip_models import get_augmentations\n",
    "from simclip_utils import is_photo, make_mosaic, load_config\n",
    "\n",
    "basic_transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.RandomResizedCrop((224, 224), scale=(0.8, 1.0)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomRotation(degrees=10),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.1, hue=0.05),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "config_path = './config.yml'\n",
    "config = load_config(config_path)\n",
    "rgb_root= config['train_dataset_path']\n",
    "landmarks_root= config['landmarks_output_path']\n",
    "\n",
    "dataset = MultiModalLandmarksQuadAugDataset(rgb_root, landmarks_root)\n",
    "dataloader = DataLoader(dataset, batch_size=4, shuffle=True, num_workers=4)\n",
    "\n",
    "for rgb_basic, rgb_mosaic, landmarks_basic, landmarks_mosaic, label in dataloader:\n",
    "    print(\"RGB基本版:\", rgb_basic.shape)\n",
    "    print(\"RGB 4x增强马赛克:\", rgb_mosaic.shape)\n",
    "    print(\"Landmarks基本版:\", landmarks_basic.shape)\n",
    "    print(\"Landmarks 4x增强马赛克:\", landmarks_mosaic.shape)\n",
    "    print(\"标签:\", label)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ae348a3e-f709-4dab-889f-e1fcf33ae399",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RGB 数据集长度: 30000\n",
      "Landmarks 数据集长度: 29616\n",
      "RGB 路径样例: ['class_0/0', 'class_0/1', 'class_0/10', 'class_0/100', 'class_0/1000']\n",
      "Landmarks 路径样例: ['class_0/0', 'class_0/1000', 'class_0/1001', 'class_0/1002', 'class_0/1003']\n",
      "公共路径数量: 29616\n",
      "公共路径样例: ['class_5/3706', 'class_1/425', 'class_6/3362', 'class_5/1411', 'class_3/2641']\n",
      "同步后 RGB 数据集长度: 29616\n",
      "同步后 Landmarks 数据集长度: 29616\n"
     ]
    }
   ],
   "source": [
    "dataset = MultiModalLandmarksQuadAugDataset(rgb_root, landmarks_root)\n",
    "train_dataloader = DataLoader(dataset, batch_size=32, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c60fff75-d70f-4fb5-9acb-060451f73d75",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/peiyuan/miniconda3/envs/pytorch-gpu/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/peiyuan/miniconda3/envs/pytorch-gpu/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision.models as models\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from pathlib import Path\n",
    "import cv2\n",
    "import albumentations as A\n",
    "import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class ResNetFeatureExtractor(nn.Module):\n",
    "    def __init__(self, input_channels):\n",
    "        super(ResNetFeatureExtractor, self).__init__()\n",
    "        if input_channels == 3:\n",
    "            self.resnet = models.resnet50(pretrained=True)\n",
    "        else:\n",
    "            self.resnet = models.resnet50(pretrained=True)\n",
    "            self.resnet.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.resnet = nn.Sequential(*list(self.resnet.children())[:-1])  # 移除最后的全连接层\n",
    "        self.fc = nn.Linear(2048, 256)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.resnet(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "# Define two networks for RGB and depth images\n",
    "rgb_net = ResNetFeatureExtractor(input_channels=3).to(device)\n",
    "depth_net = ResNetFeatureExtractor(input_channels=3).to(device)\n",
    "\n",
    "class ProjectionHead(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(ProjectionHead, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, input_dim)\n",
    "        self.bn1 = nn.BatchNorm1d(input_dim)\n",
    "        self.fc2 = nn.Linear(input_dim, input_dim)\n",
    "        self.bn2 = nn.BatchNorm1d(input_dim)\n",
    "        self.fc3 = nn.Linear(input_dim, output_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "projection_rgb = ProjectionHead(256, 128).to(device)\n",
    "projection_depth = ProjectionHead(256, 128).to(device)\n",
    "\n",
    "\n",
    "def contrastive_loss(features1, features2):\n",
    "    features1 = F.normalize(features1, p=2, dim=1)\n",
    "    features2 = F.normalize(features2, p=2, dim=1)\n",
    "    logits = torch.matmul(features1, features2.T)\n",
    "    labels = torch.arange(features1.size(0)).to(features1.device)\n",
    "    loss = F.cross_entropy(logits, labels)\n",
    "    return loss\n",
    "\n",
    "def nt_xent_loss(features1, features2, temperature=0.5):\n",
    "    features1 = F.normalize(features1, p=2, dim=1)\n",
    "    features2 = F.normalize(features2, p=2, dim=1)\n",
    "    features = torch.cat([features1, features2], dim=0)\n",
    "    similarity_matrix = torch.matmul(features, features.T) / temperature\n",
    "    batch_size = features1.size(0)\n",
    "    labels = torch.arange(batch_size).to(features1.device)\n",
    "    labels = torch.cat([labels, labels], dim=0)\n",
    "    mask = torch.eye(2 * batch_size, dtype=bool).to(features1.device)\n",
    "    similarity_matrix = similarity_matrix.masked_fill(mask, -float('inf'))\n",
    "    positives = torch.cat([torch.diag(similarity_matrix, batch_size), torch.diag(similarity_matrix, -batch_size)])\n",
    "    negatives = similarity_matrix[~mask].view(2 * batch_size, -1)\n",
    "    logits = torch.cat([positives.unsqueeze(1), negatives], dim=1)\n",
    "    labels = torch.zeros(2 * batch_size, dtype=torch.long).to(features1.device)\n",
    "    loss = F.cross_entropy(logits, labels)\n",
    "    return loss\n",
    "\n",
    "\n",
    "lambda_recon = 1.0\n",
    "optimizer = optim.Adam([\n",
    "    {'params': rgb_net.parameters()},\n",
    "    {'params': depth_net.parameters()},\n",
    "    {'params': projection_rgb.parameters()},\n",
    "    {'params': projection_depth.parameters()},\n",
    "], lr=1e-4)\n",
    "\n",
    "criterion_reconstruction = nn.MSELoss()\n",
    "\n",
    "# 后续训练流程保持不变\n",
    "# ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9397e44d-0126-441e-90c8-aa3c8dcf2622",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RGB数据集长度: 4000\n",
      "数据集长度: 4000\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import os\n",
    "from PIL import Image\n",
    "import torchvision.datasets as datasets\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "class MultiModalDataset_Test_Single(Dataset):\n",
    "    def __init__(self, rgb_root, transform_rgb=None, transform_gray=None):\n",
    "        # 使用单个数据集目录\n",
    "        self.rgb_dataset = datasets.ImageFolder(root=rgb_root)\n",
    "\n",
    "        # 用户提供的transform，用于后续处理\n",
    "        self.transform_rgb = transform_rgb\n",
    "        self.transform_gray = transform_gray\n",
    "\n",
    "        print(\"RGB数据集长度:\", len(self.rgb_dataset))\n",
    "        \n",
    "        # 数据集长度\n",
    "        self.dataset_length = len(self.rgb_dataset)\n",
    "        print(\"数据集长度:\", self.dataset_length)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.dataset_length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # 从RGB数据集中获取图像和标签\n",
    "        img_path, label = self.rgb_dataset.samples[idx]\n",
    "        \n",
    "        # 打开图像（RGB格式）\n",
    "        image_pil = Image.open(img_path).convert('RGB')\n",
    "\n",
    "        # 对RGB图像应用transform_rgb\n",
    "        if self.transform_rgb is not None:\n",
    "            rgb_image = self.transform_rgb(image_pil)\n",
    "        else:\n",
    "            # 如果未指定transform_rgb，则默认转换为Tensor\n",
    "            from torchvision import transforms\n",
    "            rgb_image = transforms.ToTensor()(image_pil)\n",
    "\n",
    "        # 对同一张图像应用transform_gray，用于生成灰度版\n",
    "        if self.transform_gray is not None:\n",
    "            gray_image = self.transform_gray(image_pil)\n",
    "        else:\n",
    "            # 如果未指定transform_gray，则简单转换为灰度后ToTensor\n",
    "            from torchvision import transforms\n",
    "            gray_transform = transforms.Compose([\n",
    "                transforms.Grayscale(num_output_channels=1),\n",
    "                transforms.ToTensor()\n",
    "            ])\n",
    "            gray_image = gray_transform(image_pil)\n",
    "        \n",
    "        # 返回RGB图像、灰度图像和标签\n",
    "        return rgb_image, gray_image, label\n",
    "\n",
    "# 定义RGB和灰度transform\n",
    "transform_rgb = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "transform_gray = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.Grayscale(num_output_channels=1),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "rgb_root= config['test_dataset_path']\n",
    "test_dataset = MultiModalDataset_Test_Single(rgb_root, transform_rgb=transform_rgb, transform_gray=transform_gray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c3dbf6cd-7c45-4a19-8860-d3a9ed15c5e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_modal_loader_test = DataLoader(test_dataset, batch_size=128, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1f91e7b4-a9ac-4c20-8bdb-7c738b3cea2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model from epoch loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "checkpoint = torch.load(f'contrastive_learning_model_epoch_{8}.pth')\n",
    "rgb_net.load_state_dict(checkpoint['rgb_net_state_dict'])\n",
    "depth_net.load_state_dict(checkpoint['depth_net_state_dict'])\n",
    "projection_rgb.load_state_dict(checkpoint['projection_rgb_state_dict'])\n",
    "projection_depth.load_state_dict(checkpoint['projection_depth_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "# rgb_decoder.load_state_dict(checkpoint['rgb_decoder_state_dict'])\n",
    "# depth_decoder.load_state_dict(checkpoint['depth_decoder_state_dict'])\n",
    "print(f'Model from epoch loaded successfully.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1103be96-c4f2-44a8-83e3-1f3a535f3beb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [1/21]:   3%|▎         | 27/926 [00:13<07:26,  2.02it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 46\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[38;5;66;03m# Backpropagation and optimization\u001b[39;00m\n\u001b[1;32m     45\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 46\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     47\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m], Average Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch_loss\u001b[38;5;250m \u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mlen\u001b[39m(train_dataloader)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch-gpu/lib/python3.11/site-packages/torch/_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    580\u001b[0m     )\n\u001b[0;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch-gpu/lib/python3.11/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch-gpu/lib/python3.11/site-packages/torch/autograd/graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Training\n",
    "rgb_net.train()\n",
    "depth_net.train()\n",
    "projection_rgb.train()\n",
    "projection_depth.train()\n",
    "\n",
    "epochs = 21\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    epoch_loss = 0\n",
    "    for rgb_images, rgb_augmented_images, depth_images, depth_augmented_images, labelq in tqdm(train_dataloader, desc=f\"Epoch [{epoch+1}/{epochs}]\"):\n",
    "        # Move data to GPU\n",
    "        rgb_images = rgb_images.to(device)\n",
    "        rgb_augmented_images = rgb_augmented_images.to(device)\n",
    "        depth_images = depth_images.to(device)\n",
    "        depth_augmented_images = depth_augmented_images.to(device)\n",
    "\n",
    "        # Contrastive learning with augmented images (RGB)\n",
    "        rgb_features_1 = rgb_net(rgb_augmented_images)\n",
    "        rgb_features_2 = rgb_net(rgb_images)\n",
    "        rgb_projection_1 = projection_rgb(rgb_features_1)\n",
    "        rgb_projection_2 = projection_rgb(rgb_features_2)\n",
    "        loss_rgb = nt_xent_loss(rgb_features_1, rgb_features_2)\n",
    "\n",
    "        # Contrastive learning with augmented images (Depth)\n",
    "        depth_features_1 = depth_net(depth_augmented_images)\n",
    "        depth_features_2 = depth_net(depth_images)\n",
    "        depth_projection_1 = projection_depth(depth_features_1)\n",
    "        depth_projection_2 = projection_depth(depth_features_2)\n",
    "        loss_depth = nt_xent_loss(depth_features_1, depth_features_2)\n",
    "\n",
    "        # Aligning RGB and depth images through contrastive learning\n",
    "        rgb_features = rgb_net(rgb_images)\n",
    "        depth_features = depth_net(depth_images)\n",
    "        rgb_projection = projection_rgb(rgb_features)\n",
    "        depth_projection = projection_depth(depth_features)\n",
    "        loss_rgb_depth = contrastive_loss(rgb_projection, depth_projection)\n",
    "\n",
    "        loss_rgb_depth_aug = contrastive_loss(rgb_projection_1, depth_projection_1)\n",
    "\n",
    "        loss = loss_rgb + loss_depth + loss_rgb_depth + loss_rgb_depth_aug\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "        # Backpropagation and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{epochs}], Average Loss: {epoch_loss / len(train_dataloader)}')\n",
    "\n",
    "    if epoch % 4 == 0 and epoch != 0:\n",
    "        # Save the model\n",
    "        torch.save({\n",
    "            'rgb_net_state_dict': rgb_net.state_dict(),\n",
    "            'depth_net_state_dict': depth_net.state_dict(),\n",
    "            'projection_rgb_state_dict': projection_rgb.state_dict(),\n",
    "            'projection_depth_state_dict': projection_depth.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "        }, f'contrastive_learning_model_epoch_{epoch}.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8681459a-781a-4dab-88a2-41840557a471",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model from epoch loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "checkpoint = torch.load(f'contrastive_learning_model_epoch_{4}.pth')\n",
    "rgb_net.load_state_dict(checkpoint['rgb_net_state_dict'])\n",
    "depth_net.load_state_dict(checkpoint['depth_net_state_dict'])\n",
    "projection_rgb.load_state_dict(checkpoint['projection_rgb_state_dict'])\n",
    "projection_depth.load_state_dict(checkpoint['projection_depth_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "# rgb_decoder.load_state_dict(checkpoint['rgb_decoder_state_dict'])\n",
    "# depth_decoder.load_state_dict(checkpoint['depth_decoder_state_dict'])\n",
    "print(f'Model from epoch loaded successfully.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "860fcf66-9b51-4f28-8e0c-32dcd3d3b535",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RGB 数据集长度: 3201\n",
      "Landmarks 数据集长度: 3216\n",
      "同步后的数据集长度: 3190\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import albumentations as A\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "\n",
    "def is_jpg(file_path):\n",
    "    return file_path.lower().endswith('.jpg')\n",
    "\n",
    "# 定义四个增强管线（针对RGB）\n",
    "transform_1 = A.Compose([\n",
    "    A.RandomResizedCrop(height=224, width=224, scale=(0.8,1.0), p=1),\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.1, hue=0.05, p=0.5),\n",
    "    A.RandomGamma(gamma_limit=(80,120), p=0.5),\n",
    "    A.Solarize(threshold=128, p=0.3),\n",
    "    A.ChannelShuffle(p=0.2),\n",
    "    A.GaussianBlur(blur_limit=(3,7), p=0.3),\n",
    "    A.JpegCompression(quality_lower=50, quality_upper=100, p=0.3),\n",
    "    A.Resize(224,224),\n",
    "])\n",
    "\n",
    "transform_2 = A.Compose([\n",
    "    A.RandomResizedCrop(height=224, width=224, scale=(0.8,1.0), p=1),\n",
    "    A.Rotate(limit=20, p=0.5),\n",
    "    A.Crop(x_min=0, y_min=0, x_max=224, y_max=112, p=1),\n",
    "    A.Resize(224,224),\n",
    "    A.InvertImg(p=0.2),\n",
    "    A.Sharpen(alpha=(0.2,0.5), p=0.3),\n",
    "    A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.5)\n",
    "])\n",
    "\n",
    "transform_3 = A.Compose([\n",
    "    A.RandomResizedCrop(height=224, width=224, scale=(0.8,1.0), p=1),\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.Crop(x_min=0, y_min=112, x_max=224, y_max=224, p=1),\n",
    "    A.Resize(224,224),\n",
    "    A.RandomGamma(gamma_limit=(80,120), p=0.5),\n",
    "    A.OpticalDistortion(distort_limit=0.2, shift_limit=0.1, p=0.5),\n",
    "    A.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.2, hue=0.1, p=0.5),\n",
    "    A.GaussianBlur(blur_limit=(3,7), p=0.3),\n",
    "])\n",
    "\n",
    "transform_4 = A.Compose([\n",
    "    A.RandomResizedCrop(height=224, width=224, scale=(0.8,1.0), p=1),\n",
    "    A.Rotate(limit=15, p=0.5),\n",
    "    A.Crop(x_min=0, y_min=56, x_max=224, y_max=168, p=1),\n",
    "    A.Resize(224,224),\n",
    "    A.Cutout(num_holes=1, max_h_size=30, max_w_size=30, fill_value=0, p=0.5),\n",
    "    A.Solarize(threshold=128, p=0.3),\n",
    "    A.JpegCompression(quality_lower=30, quality_upper=100, p=0.3),\n",
    "    A.ChannelShuffle(p=0.2),\n",
    "])\n",
    "\n",
    "\n",
    "# 基础转换(无增强)\n",
    "basic_transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),  # 调整到较大尺寸\n",
    "    transforms.RandomResizedCrop((224, 224), scale=(0.8, 1.0)),  # 随机裁剪到目标尺寸\n",
    "    transforms.RandomHorizontalFlip(p=0.5),  # 随机水平翻转\n",
    "    transforms.RandomRotation(degrees=10),  # 随机旋转±10°\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.1, hue=0.05),  # 随机亮度、对比度等\n",
    "    transforms.ToTensor(),  # 转为张量\n",
    "])\n",
    "\n",
    "def make_mosaic(images):\n",
    "    # images为4张增广后的图像(np.array格式, shape为(H,W,C))\n",
    "    row1 = np.hstack([images[0], images[1]])\n",
    "    row2 = np.hstack([images[2], images[3]])\n",
    "    mosaic = np.vstack([row1, row2])\n",
    "    return mosaic\n",
    "\n",
    "class MultiModalLandmarksQuadAugDatasetTest(Dataset):\n",
    "    def __init__(self, rgb_root, landmarks_root):\n",
    "        self.rgb_dataset = datasets.ImageFolder(root=rgb_root, is_valid_file=is_jpg)\n",
    "        self.depth_dataset = datasets.ImageFolder(root=landmarks_root, is_valid_file=is_jpg)\n",
    "\n",
    "        print(\"RGB 数据集长度:\", len(self.rgb_dataset))\n",
    "        print(\"Landmarks 数据集长度:\", len(self.depth_dataset))\n",
    "        \n",
    "        self.synchronize_datasets()\n",
    "        \n",
    "        # 定义增广列表\n",
    "        self.aug_list = [transform_1, transform_2, transform_3, transform_4]\n",
    "\n",
    "    def synchronize_datasets(self):\n",
    "        def get_relative_paths(dataset, is_depth=False):\n",
    "            relative_paths = []\n",
    "            for path, _ in dataset.samples:\n",
    "                rel_path = os.path.relpath(path, dataset.root)\n",
    "                if is_depth:\n",
    "                    dir_name, file_name = os.path.split(rel_path)\n",
    "                    base_name = file_name.replace('_landmarks', '')\n",
    "                    rel_path = os.path.join(dir_name, base_name)\n",
    "                relative_paths.append(rel_path)\n",
    "            return relative_paths\n",
    "        \n",
    "        rgb_paths = get_relative_paths(self.rgb_dataset)\n",
    "        depth_paths = get_relative_paths(self.depth_dataset, is_depth=True)\n",
    "        \n",
    "        common_paths = set(rgb_paths) & set(depth_paths)\n",
    "        \n",
    "        def filter_dataset(dataset, common_paths, is_depth=False):\n",
    "            new_samples = []\n",
    "            for path, label in dataset.samples:\n",
    "                rel_path = os.path.relpath(path, dataset.root)\n",
    "                if is_depth:\n",
    "                    dir_name, file_name = os.path.split(rel_path)\n",
    "                    base_name = file_name.replace('_landmarks', '')\n",
    "                    rel_path_processed = os.path.join(dir_name, base_name)\n",
    "                else:\n",
    "                    rel_path_processed = rel_path\n",
    "                if rel_path_processed in common_paths:\n",
    "                    new_samples.append((path, label))\n",
    "            dataset.samples = new_samples\n",
    "            dataset.targets = [s[1] for s in new_samples]\n",
    "        \n",
    "        filter_dataset(self.rgb_dataset, common_paths)\n",
    "        filter_dataset(self.depth_dataset, common_paths, is_depth=True)\n",
    "        \n",
    "        self.dataset_length = len(self.rgb_dataset)\n",
    "        print(\"同步后的数据集长度:\", self.dataset_length)\n",
    "\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.dataset_length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        rgb_path, label = self.rgb_dataset.samples[idx]\n",
    "        landmarks_path, label_l = self.depth_dataset.samples[idx]\n",
    "        assert label == label_l, \"RGB和Landmarks的标签不一致\"\n",
    "        \n",
    "        # 加载原始图像\n",
    "        rgb_image_pil = Image.open(rgb_path).convert('RGB')\n",
    "        landmarks_image_pil = Image.open(landmarks_path).convert('RGB') \n",
    "\n",
    "        # 转为numpy用于Albumentations\n",
    "        rgb_np = np.array(rgb_image_pil)\n",
    "        landmarks_np = np.array(landmarks_image_pil)\n",
    "\n",
    "        # 对同一对图像应用四种不同增强\n",
    "        rgb_augs = []\n",
    "        landmarks_augs = []\n",
    "        for aug in self.aug_list:\n",
    "            out = aug(image=rgb_np, image2=landmarks_np)\n",
    "            rgb_augs.append(out['image'])\n",
    "            landmarks_augs.append(out['image2'])\n",
    "\n",
    "        # 将四个增强后的图像拼接成2x2马赛克\n",
    "        rgb_mosaic = make_mosaic(rgb_augs)\n",
    "        landmarks_mosaic = make_mosaic(landmarks_augs)\n",
    "\n",
    "        # 缩放回224x224\n",
    "        rgb_mosaic = cv2.resize(rgb_mosaic, (224,224), interpolation=cv2.INTER_LINEAR)\n",
    "        landmarks_mosaic = cv2.resize(landmarks_mosaic, (224,224), interpolation=cv2.INTER_LINEAR)\n",
    "\n",
    "\n",
    "        # 转为Tensor\n",
    "        rgb_mosaic_tensor = transforms.ToTensor()(Image.fromarray(rgb_mosaic))\n",
    "        landmarks_mosaic_tensor = transforms.ToTensor()(Image.fromarray(landmarks_mosaic))\n",
    "\n",
    "        # 基本版本（无增强）的图像\n",
    "        rgb_basic = basic_transform(rgb_image_pil)\n",
    "        landmarks_basic = basic_transform(landmarks_image_pil)\n",
    "\n",
    "        return rgb_basic, rgb_mosaic_tensor, landmarks_basic, landmarks_mosaic_tensor, label\n",
    "\n",
    "\n",
    "rgb_root='/workspace/2470_Final/affectnet/AffectNet/test'\n",
    "landmarks_root='/workspace/2470_Final/affectnet/AffectNet_landmarks/test'\n",
    "\n",
    "test_dataset = MultiModalLandmarksQuadAugDatasetTest(rgb_root, landmarks_root)\n",
    "multi_modal_loader_test = DataLoader(test_dataset, batch_size=32, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3d42bfc4-1376-469a-895e-e13bc2531f31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Finetune Phase...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [1/12]: 100%|██████████| 1169/1169 [04:10<00:00,  4.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finetune Epoch [1/12], Average Loss: 0.0009634967947536498\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [1/12]: 100%|██████████| 100/100 [00:22<00:00,  4.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy after Finetune Epoch [1/12]: 50.41%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [2/12]: 100%|██████████| 1169/1169 [03:56<00:00,  4.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finetune Epoch [2/12], Average Loss: 0.0008749054271609891\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [2/12]: 100%|██████████| 100/100 [00:21<00:00,  4.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy after Finetune Epoch [2/12]: 52.88%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [3/12]: 100%|██████████| 1169/1169 [03:56<00:00,  4.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finetune Epoch [3/12], Average Loss: 0.0011911044475663105\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [3/12]: 100%|██████████| 100/100 [00:23<00:00,  4.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy after Finetune Epoch [3/12]: 53.45%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [4/12]: 100%|██████████| 1169/1169 [03:59<00:00,  4.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finetune Epoch [4/12], Average Loss: 0.0011400759373086451\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [4/12]: 100%|██████████| 100/100 [00:22<00:00,  4.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy after Finetune Epoch [4/12]: 54.01%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [5/12]: 100%|██████████| 1169/1169 [03:57<00:00,  4.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finetune Epoch [5/12], Average Loss: 0.0008627169146509227\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [5/12]: 100%|██████████| 100/100 [00:22<00:00,  4.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy after Finetune Epoch [5/12]: 56.43%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [6/12]: 100%|██████████| 1169/1169 [03:53<00:00,  5.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finetune Epoch [6/12], Average Loss: 0.0009461959475018817\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [6/12]: 100%|██████████| 100/100 [00:21<00:00,  4.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy after Finetune Epoch [6/12]: 56.39%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [7/12]: 100%|██████████| 1169/1169 [03:58<00:00,  4.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finetune Epoch [7/12], Average Loss: 0.0006219077762933344\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [7/12]: 100%|██████████| 100/100 [00:22<00:00,  4.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy after Finetune Epoch [7/12]: 55.64%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [8/12]: 100%|██████████| 1169/1169 [03:59<00:00,  4.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finetune Epoch [8/12], Average Loss: 0.0010079274940327562\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [8/12]: 100%|██████████| 100/100 [00:21<00:00,  4.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy after Finetune Epoch [8/12]: 56.11%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [9/12]: 100%|██████████| 1169/1169 [03:58<00:00,  4.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finetune Epoch [9/12], Average Loss: 0.0006501876563471133\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [9/12]: 100%|██████████| 100/100 [00:21<00:00,  4.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy after Finetune Epoch [9/12]: 54.39%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [10/12]:   6%|▌         | 68/1169 [00:14<03:57,  4.64it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 90\u001b[0m\n\u001b[1;32m     88\u001b[0m     \u001b[38;5;66;03m# 反向传播和优化\u001b[39;00m\n\u001b[1;32m     89\u001b[0m     finetune_optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 90\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     91\u001b[0m     finetune_optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFinetune Epoch [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfinetune_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m], Average Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch_loss\u001b[38;5;250m \u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mlen\u001b[39m(train_dataloader)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/_tensor.py:488\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    478\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    479\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    480\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    481\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    486\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    487\u001b[0m     )\n\u001b[0;32m--> 488\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/autograd/__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    192\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    194\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Finetune 阶段\n",
    "print(\"Starting Finetune Phase...\")\n",
    "finetune_epochs = 12\n",
    "\n",
    "# 定义设备\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "classifier = nn.Sequential(\n",
    "    nn.Linear(1280, 512),\n",
    "    nn.BatchNorm1d(512),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(512, 128),\n",
    "    nn.BatchNorm1d(128),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(128, 8)  # 假设有8个类别\n",
    ").to(device)\n",
    "\n",
    "class Post_projectionHead(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(Post_projectionHead, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, input_dim)\n",
    "        self.bn1 = nn.BatchNorm1d(input_dim)\n",
    "        self.fc2 = nn.Linear(input_dim, output_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "rgb_post = Post_projectionHead(512, 128)\n",
    "depth_post = Post_projectionHead(512, 128)\n",
    "rgb_post = rgb_post.to(device)\n",
    "depth_post = depth_post.to(device)\n",
    "    \n",
    "# classifier = nn.Sequential(\n",
    "#     nn.Linear(1280, 512),\n",
    "#     nn.ReLU(),\n",
    "#     nn.Linear(512, 8)  # 假设有8个类别\n",
    "# ).to(device)\n",
    "\n",
    "finetune_optimizer = optim.Adam(\n",
    "    list(rgb_net.parameters()) +\n",
    "    list(depth_net.parameters()) +\n",
    "    list(classifier.parameters()) +\n",
    "    list(projection_rgb.parameters())\n",
    "    + list(projection_depth.parameters()),\n",
    "    lr=1e-4\n",
    ")\n",
    "\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch in range(finetune_epochs):\n",
    "    epoch_loss = 0\n",
    "    for rgb_images, rgb_augmented_images, depth_images, depth_augmented_images, label_y in tqdm(train_dataloader, desc=f\"Epoch [{epoch+1}/{finetune_epochs}]\"):\n",
    "        # 将数据移动到GPU\n",
    "        rgb_images = rgb_images.to(device)\n",
    "        depth_images = depth_images.to(device)\n",
    "        rgb_augmented_images = rgb_augmented_images.to(device)\n",
    "        depth_augmented_images = depth_augmented_images.to(device)\n",
    "        label_y = label_y.to(device)\n",
    "\n",
    "        # 提取投影特征并 concatenate\n",
    "        rgb_features = rgb_net(rgb_images)\n",
    "        rgb_features_aug = rgb_net(rgb_augmented_images)\n",
    "        depth_features = depth_net(depth_images)\n",
    "        depth_features_aug = depth_net(depth_augmented_images)\n",
    "        rgb_features_2 = projection_rgb(rgb_features)\n",
    "        rgb_features_2_aug = projection_rgb(rgb_features_aug)\n",
    "        depth_features_2 = projection_depth(depth_features)\n",
    "        depth_features_2_aug = projection_depth(depth_features_aug)\n",
    "\n",
    "\n",
    "\n",
    "        combined_features = torch.cat((rgb_features_aug, rgb_features_2_aug, rgb_features, rgb_features_2, depth_features, depth_features_2, depth_features_2_aug), dim=1)\n",
    "\n",
    "        # 通过分类器进行分类\n",
    "        outputs = classifier(combined_features)\n",
    "\n",
    "        cls_loss = criterion(outputs, label_y)\n",
    "        # loss = cls_loss + recon_loss_rgb + recon_loss_depth\n",
    "        loss = cls_loss\n",
    "        epoch_loss = cls_loss.item()\n",
    "\n",
    "        # 反向传播和优化\n",
    "        finetune_optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        finetune_optimizer.step()\n",
    "    print(f'Finetune Epoch [{epoch+1}/{finetune_epochs}], Average Loss: {epoch_loss / len(train_dataloader)}')\n",
    "\n",
    "    # 测试集上的评估\n",
    "    rgb_net.eval()\n",
    "    depth_net.eval()\n",
    "    classifier.eval()\n",
    "    projection_rgb.eval()\n",
    "    projection_depth.eval()\n",
    "    rgb_post.eval()\n",
    "    depth_post.eval()\n",
    "    \n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for rgb_images_test, rgb_augmented_test, depth_images_test, depth_augmented_test, label_y_test in tqdm(multi_modal_loader_test, desc=f\"Epoch [{epoch+1}/{finetune_epochs}]\"):\n",
    "            # 将数据移动到GPU\n",
    "            rgb_images_test = rgb_images_test.to(device)\n",
    "            depth_images_test = depth_images_test.to(device)\n",
    "            rgb_augmented_test = rgb_augmented_test.to(device)\n",
    "            depth_augmented_test = depth_augmented_test.to(device)\n",
    "            label_y_test = label_y_test.to(device)\n",
    "\n",
    "            # 提取投影特征并 concatenate\n",
    "            rgb_features = rgb_net(rgb_images_test)\n",
    "            rgb_features_aug = rgb_net(rgb_augmented_test)\n",
    "            depth_features = depth_net(depth_images_test)\n",
    "            depth_features_aug = depth_net(depth_augmented_test)\n",
    "            rgb_features_2 = projection_rgb(rgb_features)\n",
    "            rgb_features_2_aug = projection_rgb(rgb_features_aug)\n",
    "            depth_features_2 = projection_depth(depth_features)\n",
    "            depth_features_2_aug = projection_depth(depth_features_aug)\n",
    "\n",
    "            combined_features = torch.cat((rgb_features_aug, rgb_features_2_aug, rgb_features, rgb_features_2, depth_features, depth_features_2, depth_features_2_aug), dim=1)\n",
    "\n",
    "            # 通过分类器进行分类\n",
    "            outputs = classifier(combined_features)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += label_y_test.size(0)\n",
    "            correct += (predicted == label_y_test).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f'Test Accuracy after Finetune Epoch [{epoch+1}/{finetune_epochs}]: {accuracy:.2f}%')\n",
    "\n",
    "    rgb_net.train()\n",
    "    depth_net.train()\n",
    "    classifier.train()\n",
    "    projection_rgb.train()\n",
    "    projection_depth.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
